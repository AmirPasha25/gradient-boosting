# üå≥ Gradient Boosting ‚Äì Intuition and Working (Regression)


![Gradient Boosting Visualization](assets/gradient_boosting.png)
## What is Gradient Boosting?

**Gradient Boosting** is an **ensemble learning technique** where models are added **sequentially** to form a strong predictor.  
Each new model is trained to **fix the mistakes** made by the previous models.

---

## Additive Modeling (Core Idea)

The model is built step by step:

- **Stage 1:** Model‚ÇÅ  
- **Stage 2:** Model‚ÇÅ + Model‚ÇÇ  
- **Stage 3:** Model‚ÇÅ + Model‚ÇÇ + Model‚ÇÉ  

This process is called **additive modeling**, where predictions are incrementally improved.

---

## Why Does Boosting Work?

In machine learning, we aim for:

- **Low Bias** (model is expressive enough)
- **Low Variance** (model generalizes well)

Bias and variance are inversely proportional.  
Gradient Boosting manages this tradeoff by:

- Using **simple models (weak learners)** with low variance  
- Combining many of them **sequentially** to reduce bias

---

## Gradient Descent vs Gradient Boosting

### Gradient Descent

- **Gradient:** Partial derivative of the loss function with respect to **model parameters**
- **Descent:** Moving in the opposite direction of the gradient to reduce loss

Used for **optimizing parameters**.

### Gradient Boosting

- **Gradient:** Partial derivative of the loss function with respect to **model predictions**
- These gradients are called **pseudo-residuals**
- **Boosting:** Sequentially adding weak learners to reduce these residuals

Used for **building the model itself**.

---

## Gradient Boosting for Regression (Step-by-Step)

### Given
- **X:** Input features  
- **y:** Actual target values  

---

## Step 1: Train the First Model (M‚ÇÅ)

The first model is a **constant prediction**.

For regression:
- **M‚ÇÅ prediction = mean(y)**

Example:
y = [10, 14, 16]
M‚ÇÅ = (10 + 14 + 16) / 3 = 13.33

Predictions from M‚ÇÅ: Pred‚ÇÅ = [13.33, 13.33, 13.33]

---

## Step 2: Compute Pseudo-Residuals for Model 2

Residuals represent what the first model failed to learn.
PseudoResidual‚ÇÇ = y ‚àí Pred‚ÇÅ
[10 ‚àí 13.33, 14 ‚àí 13.33, 16 ‚àí 13.33] = [‚àí3.33, 0.67, 2.67]

---

## Step 3: Train Model 2 (M‚ÇÇ)

- **Model type:** Decision Tree (weak learner)
- **Input (X):** Original features
- **Target (y):** PseudoResidual‚ÇÇ

Assume predictions:
M‚ÇÇ predictions = [‚àí3, 1, 2]

---

## Step 4: Update Predictions (Learning Rate Œ∑ = 0.1)
Pred‚ÇÇ = Pred‚ÇÅ + Œ∑ √ó M‚ÇÇ

Pred‚ÇÇ = 13.33 + 0.1 √ó [‚àí3, 1, 2] = [13.03, 13.43, 13.53]

---

## Step 5: Compute Pseudo-Residuals for Model 3
PseudoResidual‚ÇÉ = y ‚àí Pred‚ÇÇ

[10 ‚àí 13.03, 14 ‚àí 13.43, 16 ‚àí 13.53] = [‚àí3.03, 0.57, 2.47]

---

## Step 6: Train Model 3 (M‚ÇÉ)

- **Input:** Original features
- **Target:** PseudoResidual‚ÇÉ

Assume predictions:

M‚ÇÉ predictions = [‚àí3, 1, 2]

---

## Step 7: Update Predictions Again
Pred‚ÇÉ = Pred‚ÇÇ + Œ∑ √ó M‚ÇÉ
Pred‚ÇÉ = [13.03, 13.43, 13.53] + 0.1 √ó [‚àí3, 1, 2] = [12.73, 13.53, 13.73]

---

## Step 8: Generalization for K Models

At iteration **k**:
PseudoResidual‚Çñ = y ‚àí Pred‚Ççk‚àí1‚Çé

---

## Final Gradient Boosting Model
Final Prediction = M‚ÇÅ + Œ∑√óM‚ÇÇ + Œ∑√óM‚ÇÉ + ‚Ä¶ + Œ∑√óM‚Çñ

---

## One-Line Intuition (Lock This)

- **M‚ÇÅ:** Makes an average guess  
- **M‚ÇÇ:** Fixes M‚ÇÅ‚Äôs mistakes  
- **M‚ÇÉ:** Fixes remaining mistakes  
- Each new model learns the **residual errors** of the previous model
